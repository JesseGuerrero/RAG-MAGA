{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T18:27:14.529824Z",
     "start_time": "2024-08-03T18:27:11.946500Z"
    }
   },
   "source": [
    "import torch\n",
    "import gradio as gr\n",
    "import os\n",
    "from transformers import pipeline\n",
    "\n",
    "'''\n",
    "Model picker\n",
    "    -Token prompt\n",
    "Check point picker\n",
    "Stream of chats\n",
    "Export to story button\n",
    "\n",
    "-Implement saving checkpoints to json\n",
    "-Implement loading full checkpoint state\n",
    "-Implement saving previous\n",
    "-Implement Export Story\n",
    "'''\n",
    "pipe = None\n",
    "token = \"\"\n",
    "# Function to unload the model\n",
    "def unload_model():\n",
    "    global pipe\n",
    "    if 'pipe' in globals():\n",
    "        del pipe\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "def load_model(model_name):\n",
    "    global pipe, token\n",
    "    unload_model()\n",
    "    if len(token) > 0:\n",
    "        pipe = pipeline(\"text-generation\", model=model_name, device='cuda', token=token)\n",
    "        return\n",
    "    pipe = pipeline(\"text-generation\", model=model_name, device='cuda')\n",
    "        \n",
    "def changeToken(newToken):\n",
    "    global token\n",
    "    token = newToken\n",
    "\n",
    "# Close Gradio interface if open\n",
    "unload_model()\n",
    "load_model(\"Qwen/Qwen2-0.5B-Instruct\")\n",
    "if 'iface' in globals() and iface is not None:\n",
    "    iface.close()\n",
    "    \n",
    "\n",
    "# Function to handle model inference\n",
    "def llama_inference(prompt):\n",
    "    global pipe\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = pipe(messages, max_length=1024)\n",
    "    print(response)\n",
    "    return response[0]['generated_text'][1]['content']\n",
    "\n",
    "print(llama_inference('hello'))\n",
    "\n",
    "css = \"\"\"\n",
    "    textarea, .white-background input {\n",
    "        background-color: white !important;\n",
    "        color: black !important;\n",
    "        -webkit-text-fill-color: black !important;\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "checkPoints = ['CPT1', 'CPT2', 'CPT3', 'CPT4', 'CPT5', 'CPT6']\n",
    "history = []\n",
    "\n",
    "# Create the Gradio interface\n",
    "with gr.Blocks(css=css) as iface:\n",
    "    global history\n",
    "    chatbot = gr.Chatbot(elem_id=\"chatbot\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=3):\n",
    "            model_dropdown = gr.Dropdown(\n",
    "                choices=[\"Qwen/Qwen2-0.5B-Instruct\", \"meta-llama/Meta-Llama-3.1-8B\"],\n",
    "                label=\"Select Model\",\n",
    "                value=\"Qwen/Qwen2-0.5B-Instruct\",\n",
    "                interactive=True,\n",
    "                change=load_model\n",
    "            )\n",
    "            hf_token_box = gr.Textbox(placeholder=\"Enter your Hugging Face token...\", label=\"Hugging Face Token\", type=\"password\", elem_classes=[\"white-background\"])\n",
    "            \n",
    "        \n",
    "        with gr.Column(scale=7):\n",
    "            checkpoint_dropdown = gr.Dropdown(choices=checkPoints, label=\"CheckPoints\", interactive=True)\n",
    "            prompt_input = gr.Textbox(lines=2, placeholder=\"Enter your prompt here...\", label=\"Prompt\", elem_classes=[\"white-background\"])\n",
    "            submit_button = gr.Button(\"Submit\")\n",
    "            button = gr.Button(\"Export Storyline\")\n",
    "\n",
    "    def update_chatbot(prompt):\n",
    "        response = llama_inference(prompt)\n",
    "        history.append((prompt, response))\n",
    "        return history  # Returns a list of tuples (input, response)\n",
    "\n",
    "    submit_button.click(update_chatbot, [prompt_input], chatbot)\n",
    "    hf_token_box.change(changeToken, [hf_token_box], None)\n",
    "\n",
    "# Launch the Gradio app\n",
    "iface.launch()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesse\\AppData\\Local\\Temp\\ipykernel_6860\\3001120504.py:69: GradioUnusedKwargWarning: You have unused kwarg parameters in Dropdown, please remove them: {'change': <function load_model at 0x000001D90159A660>}\n",
      "  model_dropdown = gr.Dropdown(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': [{'role': 'user', 'content': 'hello'}, {'role': 'assistant', 'content': 'Hello! How can I assist you today?'}]}]\n",
      "Hello! How can I assist you today?\n",
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ed610b-6eb4-4645-95f2-5aa3ddba1568",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
