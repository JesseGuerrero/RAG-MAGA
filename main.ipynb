{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T21:17:53.506926Z",
     "start_time": "2024-08-06T21:17:50.476159Z"
    }
   },
   "source": [
    "import torch\n",
    "import gradio as gr\n",
    "import os\n",
    "from transformers import pipeline, Pipeline\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "'''\n",
    "Model picker\n",
    "    -Token prompt\n",
    "Check point picker\n",
    "Stream of chats\n",
    "Export to story button\n",
    "\n",
    "-Implement saving checkpoints to json\n",
    "-Implement loading full checkpoint state\n",
    "-Implement saving previous\n",
    "-Implement Export Story\n",
    "'''\n",
    "pipe = None\n",
    "token = \"\"\n",
    "\n",
    "# Function to unload the model\n",
    "def unload_model():\n",
    "    global pipe\n",
    "    if 'pipe' in globals():\n",
    "        del pipe\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "def load_model(model_name):\n",
    "    global pipe, token\n",
    "    unload_model()\n",
    "    if len(token) > 0:\n",
    "        pipe = pipeline(\"text-generation\", model=model_name, device='cuda', token=token)\n",
    "        return\n",
    "    pipe = pipeline(\"text-generation\", model=model_name, device='cuda')\n",
    "\n",
    "def changeToken(newToken):\n",
    "    global token\n",
    "    token = newToken\n",
    "\n",
    "# Close Gradio interface if open\n",
    "unload_model()\n",
    "load_model(\"Qwen/Qwen2-0.5B-Instruct\")\n",
    "if 'iface' in globals() and iface is not None:\n",
    "    iface.close()\n",
    "\n",
    "def llama_inference(history, new_prompt):\n",
    "    global pipe\n",
    "    # Create a combined prompt with the entire chat history\n",
    "    combined_prompt = \"\"\n",
    "    for (prev_prompt, response) in history:\n",
    "        combined_prompt += f\"User: {prev_prompt}\\nAssistant: {response}\\n\"\n",
    "    combined_prompt += f\"User: {new_prompt}\\nAssistant: \"\n",
    "    messages = [{\"role\": \"user\", \"content\": combined_prompt}]\n",
    "    response = \"\"\n",
    "    if \"Qwen\" in str(pipe.model): \n",
    "        response = pipe(messages, max_length=1024)\n",
    "    else:\n",
    "        response = pipe(messages)\n",
    "    \n",
    "    # Log response and messages for debugging\n",
    "    debug_info = f\"{messages} response: {response}\"\n",
    "    \n",
    "    # Assuming the response structure might need adjustments\n",
    "    generated_text = response[0]['generated_text']\n",
    "    if isinstance(generated_text, list):\n",
    "        content = generated_text[1]['content']  # Adjusting if the response is a list\n",
    "    else:\n",
    "        content = generated_text  # Direct access if not a list\n",
    "    \n",
    "    return content, debug_info\n",
    "\n",
    "css = \"\"\"\n",
    "    .white-background textarea, .white-background input {\n",
    "        background-color: white !important;\n",
    "        color: black !important;\n",
    "        -webkit-text-fill-color: black !important;\n",
    "    }\n",
    "    .file-preview .empty {\n",
    "        display: none;  /* Adjust the height to your preference */\n",
    "    }\n",
    "    .file-preview .full {\n",
    "        display: block;  /* Adjust the height to your preference */\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "history = []\n",
    "# Create the Gradio interface\n",
    "with gr.Blocks(css=css) as iface:\n",
    "    global history   \n",
    "    chatbot = gr.Chatbot(elem_id=\"chatbot\")      \n",
    "    checkpoint_dropdown = gr.Dropdown(choices=os.listdir(\"checkpoints\")[:-1], label=\"CheckPoints\", interactive=True)\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=3):\n",
    "            model_dropdown = gr.Dropdown(\n",
    "                choices=[\"Qwen/Qwen2-0.5B-Instruct\", \"meta-llama/Meta-Llama-3.1-8B\"],\n",
    "                label=\"Select Model\",\n",
    "                value=\"Qwen/Qwen2-0.5B-Instruct\",\n",
    "                interactive=True,\n",
    "                change=load_model\n",
    "            )\n",
    "            hf_token_box = gr.Textbox(placeholder=\"Enter your Hugging Face token...\", label=\"Hugging Face Token\", type=\"password\", elem_classes=[\"white-background\"])\n",
    "            clear_button = gr.Button(\"Clear\")\n",
    "            debug_output = gr.Textbox(lines=10, placeholder=\"Debug information will appear here...\", label=\"Debug Output\")\n",
    "            \n",
    "        with gr.Column(scale=7):\n",
    "            export_file = gr.File(label=\"Export Storyline\", elem_classes=[\"file-preview\"])            \n",
    "            prompt_input = gr.Textbox(lines=2, placeholder=\"Enter your prompt here...\", label=\"Prompt\", elem_classes=[\"white-background\"])\n",
    "            submit_button = gr.Button(\"Submit\")\n",
    "            save_chkpt_button = gr.Button(\"Save Checkpoint\")\n",
    "            download_button = gr.Button(\"Export Storyline\")\n",
    "\n",
    "    def update_chatbot(prompt):\n",
    "        global history\n",
    "        response, debug_info = llama_inference(history, prompt)\n",
    "        history.append((prompt, response))\n",
    "        return history, debug_info  # Returns a list of tuples (input, response)\n",
    "    \n",
    "    def clear_history():\n",
    "        global history\n",
    "        history.clear()\n",
    "        return history, \"Cleared chatbot\"\n",
    "    \n",
    "    def export_story():\n",
    "        file_name = datetime.now().strftime(\"%m-%d-%H-%M-%S\")\n",
    "        file_path = f\"{file_name}.txt\"\n",
    "        output = \"\"\n",
    "        for prompt, response in history:\n",
    "            output += \"---\\n\" + response + \"\\n\"\n",
    "        with open(f'story-lines/{file_path}', \"w\") as file:\n",
    "            file.write(output)\n",
    "        return file_path\n",
    "    \n",
    "    def load_chkpt(name: str):\n",
    "        global history\n",
    "        checkpoint_dropdown.update(choices=os.listdir(f\"checkpoints\")[::-1])\n",
    "        file_path = f\"checkpoints/{name}\"\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            history = json.load(json_file)\n",
    "        # Convert the history to the appropriate format for the chatbot component\n",
    "        chatbot_history = [(item[0], item[1]) for item in history]\n",
    "        checkpoint_dropdown.choices = os.listdir('checkpoints')[:-1]\n",
    "        return chatbot_history, f\"Loaded {name}\", gr.Dropdown.update(choices=os.listdir('checkpoints')[:-1])\n",
    "    \n",
    "    def save_chkpt():\n",
    "        chkpt_name = datetime.now().strftime(\"%m-%d-%H-%M-%S\")\n",
    "        file_path = f\"checkpoints/{chkpt_name}.json\"\n",
    "        with open(file_path, 'w') as json_file:\n",
    "            json.dump(history, json_file)\n",
    "        return f\"Checkpoint {chkpt_name} saved.\"\n",
    "        \n",
    "    submit_button.click(update_chatbot, [prompt_input], [chatbot, debug_output])\n",
    "    clear_button.click(clear_history, [], [chatbot, debug_output])\n",
    "    download_button.click(export_story, [], export_file)\n",
    "    save_chkpt_button.click(save_chkpt, [], [debug_output])\n",
    "\n",
    "    hf_token_box.change(changeToken, [hf_token_box], None)\n",
    "    checkpoint_dropdown.change(load_chkpt, [checkpoint_dropdown], [chatbot, debug_output, checkpoint_dropdown])\n",
    "\n",
    "# Launch the Gradio app\n",
    "iface.launch()\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "C:\\Users\\jesse\\AppData\\Local\\Temp\\ipykernel_10528\\518429486.py:97: GradioUnusedKwargWarning: You have unused kwarg parameters in Dropdown, please remove them: {'change': <function load_model at 0x0000020438664220>}\n",
      "  model_dropdown = gr.Dropdown(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n",
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ed610b-6eb4-4645-95f2-5aa3ddba1568",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
