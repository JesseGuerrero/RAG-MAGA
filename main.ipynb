{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T22:19:44.797348Z",
     "start_time": "2024-08-06T22:19:39.589379Z"
    }
   },
   "source": [
    "import torch\n",
    "import gradio as gr\n",
    "import os\n",
    "from transformers import pipeline, Pipeline\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# System and initial configurations\n",
    "token_from_system = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "pipe = None\n",
    "token = token_from_system if token_from_system is not None and len(token_from_system) > 0 else \"\"\n",
    "system_prompt = \"You are an AI story writer for a video game. You tell one story plot point based on the prompt by the user. It is a dystopian story.\"\n",
    "\n",
    "import torch\n",
    "\n",
    "def get_vram_usage_gb() -> int: # GB\n",
    "    if torch.cuda.is_available():\n",
    "        total_vram = torch.cuda.get_device_properties(0).total_memory\n",
    "        allocated_vram = torch.cuda.memory_allocated(0)\n",
    "        free_vram = total_vram - allocated_vram\n",
    "        return int(free_vram / 1024**3)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Function to unload the model\n",
    "def unload_model():\n",
    "    global pipe\n",
    "    if 'pipe' in globals():\n",
    "        del pipe\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Function to load the model\n",
    "def load_model(model_name):\n",
    "    global pipe, token\n",
    "    debug = \"\"\n",
    "    try:\n",
    "        unload_model()\n",
    "        if len(token) > 0:\n",
    "            pipe = pipeline(\"text-generation\", model=model_name, device='cuda', token=token)\n",
    "            return\n",
    "        pipe = pipeline(\"text-generation\", model=model_name, device='cuda')\n",
    "        debug = f\"Loading {model_name}...\"\n",
    "    except Exception as e:\n",
    "        debug = f\"Loading {model_name} failed: {e}\"\n",
    "    return debug\n",
    "\n",
    "# Function to change token\n",
    "def changeToken(newToken):\n",
    "    global token\n",
    "    token = newToken\n",
    "    \n",
    "def list_checkpoints_excluding_gitkeep(directory='checkpoints'):\n",
    "    files = os.listdir(directory)\n",
    "    if \"git.keep\" in files:\n",
    "        files.remove(\"git.keep\")\n",
    "    return files\n",
    "\n",
    "# Unload model if interface is closed\n",
    "unload_model()\n",
    "startingModel = \"Qwen/Qwen2-0.5B-Instruct\" \n",
    "if get_vram_usage_gb() > 30:\n",
    "    startingModel = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "load_model(startingModel)\n",
    "if 'iface' in globals() and iface is not None:\n",
    "    iface.close()\n",
    "\n",
    "# Function to perform inference using the model\n",
    "def llama_inference(history, new_prompt):\n",
    "    global pipe, system_prompt\n",
    "    combined_prompt = f\"{system_prompt}\\n\"\n",
    "    for (prev_prompt, response) in history:\n",
    "        combined_prompt += f\"User: {prev_prompt}\\nAssistant: {response}\\n\"\n",
    "    combined_prompt += f\"User: {new_prompt}\\nAssistant: \"\n",
    "    messages = [{\"role\": \"user\", \"content\": combined_prompt}]\n",
    "    response = pipe(messages, max_length=2048) if \"Qwen\" in str(pipe.model) else pipe(messages, max_length=100_000)\n",
    "    \n",
    "    generated_text = response[0]['generated_text']\n",
    "    content = generated_text[1]['content'] if isinstance(generated_text, list) else generated_text\n",
    "    \n",
    "    date = datetime.now().strftime(\"%m-%d\")\n",
    "    chkpt_name = f\"{date}-Latest\" \n",
    "    file_path = f\"checkpoints/{chkpt_name}.json\"\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump(history, json_file)\n",
    "    \n",
    "    return content, f\"{messages} response: {response}\"\n",
    "\n",
    "# CSS styles for the interface\n",
    "css = \"\"\"\n",
    "    .white-background textarea, .white-background input {\n",
    "        background-color: white !important;\n",
    "        color: black !important;\n",
    "        -webkit-text-fill-color: black !important;\n",
    "    }\n",
    "    .file-preview .empty {\n",
    "        display: none;\n",
    "    }\n",
    "    .file-preview .full {\n",
    "        display: block;\n",
    "    }\n",
    "\"\"\"\n",
    "history = []\n",
    "\n",
    "# Create the Gradio interface\n",
    "with gr.Blocks(css=css) as iface:\n",
    "    global history   \n",
    "    chatbot = gr.Chatbot(elem_id=\"chatbot\")      \n",
    "    checkpoint_dropdown = gr.Dropdown(choices=os.listdir(\"checkpoints\")[:-1], label=\"CheckPoints\", interactive=True)\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=3):\n",
    "            model_dropdown = gr.Dropdown(\n",
    "                choices=[\"Qwen/Qwen2-0.5B-Instruct\", \"meta-llama/Meta-Llama-3.1-8B-Instruct\"],\n",
    "                label=\"Select Model\",\n",
    "                value=startingModel,\n",
    "                interactive=True\n",
    "            )\n",
    "            hf_token_box = gr.Textbox(placeholder=\"Enter your Hugging Face token...\", value=token, label=\"Hugging Face Token\", type=\"password\", elem_classes=[\"white-background\"])\n",
    "            clear_button = gr.Button(\"Clear\")\n",
    "            debug_output = gr.Textbox(lines=10, placeholder=\"Debug information will appear here...\", label=\"Debug Output\")\n",
    "            \n",
    "        with gr.Column(scale=7):\n",
    "            export_file = gr.File(label=\"Export Storyline\", elem_classes=[\"file-preview\"])            \n",
    "            prompt_input = gr.Textbox(lines=2, placeholder=\"Enter your prompt here...\", label=\"Prompt\", elem_classes=[\"white-background\"])\n",
    "            submit_button = gr.Button(\"Submit\")\n",
    "            save_chkpt_button = gr.Button(\"Save Checkpoint\")\n",
    "            download_button = gr.Button(\"Export Storyline\")\n",
    "\n",
    "    # Function to update chatbot with new prompt\n",
    "    def update_chatbot(prompt):\n",
    "        global history\n",
    "        response, debug_info = llama_inference(history, prompt)\n",
    "        history.append((prompt, response))\n",
    "        return history, debug_info, gr.Dropdown.update(choices=list_checkpoints_excluding_gitkeep())\n",
    "    \n",
    "    # Function to clear history\n",
    "    def clear_history():\n",
    "        global history\n",
    "        history.clear()\n",
    "        return history, \"Cleared chatbot\"\n",
    "    \n",
    "    # Function to export the story\n",
    "    def export_story():\n",
    "        file_name = datetime.now().strftime(\"%m-%d-%H-%M-%S\")\n",
    "        file_path = f\"{file_name}.txt\"\n",
    "        output = \"\\n\".join(f\"---\\n{response}\" for _, response in history)\n",
    "        with open(f'story-lines/{file_path}', \"w\") as file:\n",
    "            file.write(output)\n",
    "        return file_path\n",
    "    \n",
    "    # Function to load a checkpoint\n",
    "    def load_chkpt(name: str):\n",
    "        global history\n",
    "        checkpoint_dropdown.update(choices=list_checkpoints_excluding_gitkeep())\n",
    "        file_path = f\"checkpoints/{name}\"\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            history = json.load(json_file)\n",
    "        chatbot_history = [(item[0], item[1]) for item in history]\n",
    "        checkpoint_dropdown.choices = os.listdir('checkpoints')[:-1]\n",
    "        return chatbot_history, f\"Loaded {name}\", gr.Dropdown.update(choices=list_checkpoints_excluding_gitkeep())\n",
    "    \n",
    "    # Function to save a checkpoint\n",
    "    def save_chkpt():\n",
    "        chkpt_name = datetime.now().strftime(\"%m-%d-%H-%M-%S\")\n",
    "        file_path = f\"checkpoints/{chkpt_name}.json\"\n",
    "        with open(file_path, 'w') as json_file:\n",
    "            json.dump(history, json_file)\n",
    "        return f\"Checkpoint {chkpt_name} saved.\"\n",
    "        \n",
    "    submit_button.click(update_chatbot, [prompt_input], [chatbot, debug_output, checkpoint_dropdown])\n",
    "    clear_button.click(clear_history, [], [chatbot, debug_output])\n",
    "    download_button.click(export_story, [], export_file)\n",
    "    save_chkpt_button.click(save_chkpt, [], [debug_output])\n",
    "\n",
    "    hf_token_box.change(changeToken, [hf_token_box], None)\n",
    "    checkpoint_dropdown.change(load_chkpt, [checkpoint_dropdown], [chatbot, debug_output, checkpoint_dropdown])\n",
    "    model_dropdown.change(load_model, [model_dropdown], [debug_output])\n",
    "\n",
    "# Launch the Gradio app\n",
    "iface.launch()\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "C:\\Users\\jesse\\AppData\\Local\\Temp\\ipykernel_10528\\3544113264.py:102: GradioUnusedKwargWarning: You have unused kwarg parameters in Blocks, please remove them: {'js': \"\\n        function loadState() {\\n            const history = JSON.parse(localStorage.getItem('chat_history'));\\n            const model = localStorage.getItem('model');\\n            if (history) {\\n                gradioApp().updateChatbot(history);\\n            }\\n            if (model) {\\n                gradioApp().updateModel(model);\\n            }\\n        }\\n        \\n        function saveState() {\\n            const history = gradioApp().chatbot.value();\\n            const model = gradioApp().model_dropdown.value();\\n            localStorage.setItem('chat_history', JSON.stringify(history));\\n            localStorage.setItem('model', model);\\n        }\\n        \\n        window.addEventListener('load', loadState);\\n        window.addEventListener('beforeunload', saveState);\\n\"}\n",
      "  with gr.Blocks(css=css, js=js) as iface:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n",
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ed610b-6eb4-4645-95f2-5aa3ddba1568",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
